{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import ExpectationMaximization as EM\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from pgmpy.inference import BeliefPropagation  # Ensure the correct import based on your setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bayesian Network structure\n",
    "model = BayesianNetwork([\n",
    "    ('length', 'theta'),\n",
    "    ('lanes', 'theta'),\n",
    "    ('max_speed', 'theta'),\n",
    "    ('dir', 'theta'),\n",
    "    ('tor', 'theta'),\n",
    "    ('n_connnections', 'theta'),\n",
    "    ('weighted_POI', 'theta'),\n",
    "    ('theta', 'Volume'),\n",
    "    ('theta', 'avg_speed'),\n",
    "    ('time', 'Volume'),\n",
    "    ('time', 'avg_speed'),\n",
    "    ('no_taxi_car', 'Volume'),\n",
    "    ('no_taxi_car', 'avg_speed std'),\n",
    "    ('avg_speed', 'avg_speed std')\n",
    "])\n",
    "\n",
    "# Set latent variables\n",
    "model.latents = {'theta', 'Volume'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"D:/Thesis/files_output_dir/output_files/\"\n",
    "road_data= pd.read_csv(output_dir +'PGM_input_discrete_level3_road_history_holiday.csv')\n",
    "data = road_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>lanes</th>\n",
       "      <th>max_speed</th>\n",
       "      <th>dir</th>\n",
       "      <th>tor</th>\n",
       "      <th>n_connnections</th>\n",
       "      <th>weighted_POI</th>\n",
       "      <th>time</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>avg_speed std</th>\n",
       "      <th>no_taxi_car</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  lanes  max_speed  dir  tor  n_connnections  weighted_POI  time  \\\n",
       "0       2      2          4    2    1               2             3     1   \n",
       "1       2      2          4    2    1               2             3     1   \n",
       "2       2      2          4    2    1               2             3     1   \n",
       "3       2      2          4    2    1               2             3     2   \n",
       "4       2      2          4    2    1               2             3     2   \n",
       "\n",
       "   avg_speed  avg_speed std  no_taxi_car  \n",
       "0          3              2            4  \n",
       "1          2              2            5  \n",
       "2          4              1            4  \n",
       "3          3              1            4  \n",
       "4          2              2            5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Thesis\\python_venv\\thesis_venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Expectation-Maximization Estimator\n",
    "estimator = EM(model, data)\n",
    "\n",
    "# Function to run a part of the EM algorithm in parallel\n",
    "def run_em_partial(estimator, data_chunk, latent_card):\n",
    "    # Each process works with a subset (chunk) of the data\n",
    "    partial_estimator = EM(estimator.model, data_chunk)\n",
    "    partial_params = partial_estimator.get_parameters(latent_card=latent_card)\n",
    "    return partial_params\n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "num_chunks = multiprocessing.cpu_count()\n",
    "data_chunks = np.array_split(data, num_chunks)\n",
    "\n",
    "# Use multiprocessing Pool to parallelize the EM algorithm\n",
    "with multiprocessing.Pool(processes=num_chunks) as pool:\n",
    "    # Partially apply the run_em_partial function with fixed arguments\n",
    "    em_partial = partial(run_em_partial, estimator, latent_card={'theta': 3, 'Volume': 5})\n",
    "    \n",
    "    # Map the function across data chunks in parallel\n",
    "    results = pool.map(em_partial, data_chunks)\n",
    "\n",
    "# Combine results from all processes (results is a list of CPD lists)\n",
    "estimated_params = []\n",
    "for result in results:\n",
    "    estimated_params.extend(result)\n",
    "\n",
    "# Add the learned CPDs back to the model\n",
    "model.add_cpds(*estimated_params)\n",
    "\n",
    "# Check if the model is valid with the added CPDs\n",
    "assert model.check_model(), \"The model is not valid with the learned CPDs.\"\n",
    "\n",
    "# Save the model using pickle (optional)\n",
    "with open(output_dir + 'bayesian_network_model_level3_holiday.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print CPDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned CPD for weighted_POI:\n",
      "+-----------------+-----------+\n",
      "| weighted_POI(1) | 0.527377  |\n",
      "+-----------------+-----------+\n",
      "| weighted_POI(2) | 0.268126  |\n",
      "+-----------------+-----------+\n",
      "| weighted_POI(3) | 0.117407  |\n",
      "+-----------------+-----------+\n",
      "| weighted_POI(4) | 0.0614906 |\n",
      "+-----------------+-----------+\n",
      "| weighted_POI(5) | 0.0255983 |\n",
      "+-----------------+-----------+\n",
      "\n",
      "\n",
      "Learned CPD for tor:\n",
      "+--------+-----------+\n",
      "| tor(1) | 0.975091  |\n",
      "+--------+-----------+\n",
      "| tor(2) | 0.0249095 |\n",
      "+--------+-----------+\n",
      "\n",
      "\n",
      "Learned CPD for no_taxi_car:\n",
      "+----------------+----------+\n",
      "| no_taxi_car(4) | 0.259895 |\n",
      "+----------------+----------+\n",
      "| no_taxi_car(5) | 0.300038 |\n",
      "+----------------+----------+\n",
      "| no_taxi_car(6) | 0.203752 |\n",
      "+----------------+----------+\n",
      "| no_taxi_car(7) | 0.236315 |\n",
      "+----------------+----------+\n",
      "\n",
      "\n",
      "Learned CPD for max_speed:\n",
      "+--------------+-------------+\n",
      "| max_speed(4) | 0.846426    |\n",
      "+--------------+-------------+\n",
      "| max_speed(5) | 0.000156763 |\n",
      "+--------------+-------------+\n",
      "| max_speed(6) | 0.153417    |\n",
      "+--------------+-------------+\n",
      "\n",
      "\n",
      "Learned CPD for time:\n",
      "+---------+----------+\n",
      "| time(1) | 0.166186 |\n",
      "+---------+----------+\n",
      "| time(2) | 0.148459 |\n",
      "+---------+----------+\n",
      "| time(3) | 0.309817 |\n",
      "+---------+----------+\n",
      "| time(4) | 0.209074 |\n",
      "+---------+----------+\n",
      "| time(5) | 0.166463 |\n",
      "+---------+----------+\n",
      "\n",
      "\n",
      "Learned CPD for lanes:\n",
      "+----------+-----------+\n",
      "| lanes(1) | 0.534915  |\n",
      "+----------+-----------+\n",
      "| lanes(2) | 0.389147  |\n",
      "+----------+-----------+\n",
      "| lanes(3) | 0.0759382 |\n",
      "+----------+-----------+\n",
      "\n",
      "\n",
      "Learned CPD for n_connnections:\n",
      "+-------------------+----------+\n",
      "| n_connnections(1) | 0.13935  |\n",
      "+-------------------+----------+\n",
      "| n_connnections(2) | 0.564399 |\n",
      "+-------------------+----------+\n",
      "| n_connnections(3) | 0.296252 |\n",
      "+-------------------+----------+\n",
      "\n",
      "\n",
      "Learned CPD for avg_speed std:\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed        | ... | avg_speed(6)           |\n",
      "+------------------+-----+------------------------+\n",
      "| no_taxi_car      | ... | no_taxi_car(7)         |\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed std(1) | ... | 0.042423907639617744   |\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed std(2) | ... | 0.29299011213611004    |\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed std(3) | ... | 0.4279953598851019     |\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed std(4) | ... | 0.2184720764514169     |\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed std(5) | ... | 0.017842346572391317   |\n",
      "+------------------+-----+------------------------+\n",
      "| avg_speed std(6) | ... | 0.00027619731536209467 |\n",
      "+------------------+-----+------------------------+\n",
      "\n",
      "\n",
      "Learned CPD for dir:\n",
      "+--------+----------+\n",
      "| dir(1) | 0.372937 |\n",
      "+--------+----------+\n",
      "| dir(2) | 0.627063 |\n",
      "+--------+----------+\n",
      "\n",
      "\n",
      "Learned CPD for length:\n",
      "+-----------+----------+\n",
      "| length(1) | 0.344994 |\n",
      "+-----------+----------+\n",
      "| length(2) | 0.655006 |\n",
      "+-----------+----------+\n",
      "\n",
      "\n",
      "Learned CPD for avg_speed:\n",
      "+--------------+-----+----------------------+\n",
      "| theta        | ... | theta(2)             |\n",
      "+--------------+-----+----------------------+\n",
      "| time         | ... | time(5)              |\n",
      "+--------------+-----+----------------------+\n",
      "| avg_speed(1) | ... | 0.001893852278553285 |\n",
      "+--------------+-----+----------------------+\n",
      "| avg_speed(2) | ... | 0.003979743546067041 |\n",
      "+--------------+-----+----------------------+\n",
      "| avg_speed(3) | ... | 0.01695625076627861  |\n",
      "+--------------+-----+----------------------+\n",
      "| avg_speed(4) | ... | 0.07718553873916935  |\n",
      "+--------------+-----+----------------------+\n",
      "| avg_speed(5) | ... | 0.22406939752468039  |\n",
      "+--------------+-----+----------------------+\n",
      "| avg_speed(6) | ... | 0.6759152171452514   |\n",
      "+--------------+-----+----------------------+\n",
      "\n",
      "\n",
      "Learned CPD for theta:\n",
      "+----------------+-----+--------------------+\n",
      "| dir            | ... | dir(2)             |\n",
      "+----------------+-----+--------------------+\n",
      "| lanes          | ... | lanes(3)           |\n",
      "+----------------+-----+--------------------+\n",
      "| length         | ... | length(2)          |\n",
      "+----------------+-----+--------------------+\n",
      "| max_speed      | ... | max_speed(6)       |\n",
      "+----------------+-----+--------------------+\n",
      "| n_connnections | ... | n_connnections(3)  |\n",
      "+----------------+-----+--------------------+\n",
      "| tor            | ... | tor(2)             |\n",
      "+----------------+-----+--------------------+\n",
      "| weighted_POI   | ... | weighted_POI(5)    |\n",
      "+----------------+-----+--------------------+\n",
      "| theta(0)       | ... | 0.3333333333333333 |\n",
      "+----------------+-----+--------------------+\n",
      "| theta(1)       | ... | 0.3333333333333333 |\n",
      "+----------------+-----+--------------------+\n",
      "| theta(2)       | ... | 0.3333333333333333 |\n",
      "+----------------+-----+--------------------+\n",
      "\n",
      "\n",
      "Learned CPD for Volume:\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| no_taxi_car | no_taxi_car(4)      | ... | no_taxi_car(7)       |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| theta       | theta(0)            | ... | theta(2)             |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| time        | time(1)             | ... | time(5)              |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| Volume(0)   | 0.19531458933862644 | ... | 0.2883856157276681   |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| Volume(1)   | 0.2534634589473476  | ... | 0.16950916274498873  |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| Volume(2)   | 0.3056559856532483  | ... | 0.005392448840633474 |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| Volume(3)   | 0.15589097510613495 | ... | 0.18217051127637268  |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "| Volume(4)   | 0.08967499095464257 | ... | 0.354542261410337    |\n",
      "+-------------+---------------------+-----+----------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the learned CPDs\n",
    "for cpd in model.get_cpds():\n",
    "    print(f\"Learned CPD for {cpd.variable}:\")\n",
    "    print(cpd)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open(output_dir + 'bayesian_network_model_level3_holiday.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "# Initialize new columns for inferred latent variables in the dataset\n",
    "road_data['theta'] = np.nan\n",
    "road_data['Volume'] = np.nan\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Function to process a chunk of rows and perform inference.\"\"\"\n",
    "    results = []\n",
    "    for index, row in chunk.iterrows():\n",
    "        # Extract observed evidence from the current row as a dictionary\n",
    "        evidence = {\n",
    "            'length': row['length'],\n",
    "            'lanes': row['lanes'],\n",
    "            'max_speed': row['max_speed'],\n",
    "            'dir': row['dir'],\n",
    "            'tor': row['tor'],\n",
    "            'n_connnections': row['n_connnections'],\n",
    "            'weighted_POI': row['weighted_POI'],\n",
    "            'time': row['time'],\n",
    "            'no_taxi_car': row['no_taxi_car'],\n",
    "            'avg_speed': row['avg_speed'],\n",
    "            'avg_speed std': row['avg_speed std']\n",
    "        }\n",
    "        \n",
    "        # Remove NaN values from the evidence dictionary\n",
    "        evidence = {k: v for k, v in evidence.items() if not pd.isna(v)}\n",
    "\n",
    "        # Perform MAP query to find the most likely value of latent nodes\n",
    "        try:\n",
    "            result_theta = inference.map_query(variables=['theta'], evidence=evidence)\n",
    "            result_volume = inference.map_query(variables=['Volume'], evidence=evidence)\n",
    "        except Exception as e:\n",
    "            # If inference fails, return NaNs\n",
    "            result_theta = {'theta': np.nan}\n",
    "            result_volume = {'Volume': np.nan}\n",
    "\n",
    "        # Collect the results\n",
    "        results.append((index, result_theta.get('theta', np.nan), result_volume.get('Volume', np.nan)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Split data into chunks for parallel processing\n",
    "num_chunks = multiprocessing.cpu_count()\n",
    "data_chunks = np.array_split(road_data, num_chunks)\n",
    "\n",
    "# Use multiprocessing Pool to parallelize the inference process\n",
    "with multiprocessing.Pool(processes=num_chunks) as pool:\n",
    "    # Map the process_chunk function across the data chunks in parallel\n",
    "    chunk_results = pool.map(process_chunk, data_chunks)\n",
    "\n",
    "# Flatten the list of results\n",
    "results = [item for sublist in chunk_results for item in sublist]\n",
    "\n",
    "# Update the DataFrame with results\n",
    "for index, theta, volume in results:\n",
    "    road_data.at[index, 'theta'] = theta\n",
    "    road_data.at[index, 'Volume'] = volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the updated DataFrame\n",
    "road_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:/Thesis/files_output_dir/output_files/level3_road_history_holiday_inference.csv\"\n",
    "road_data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:/Thesis/files_output_dir/output_files/level2_road_history_workday_inference.csv\"\n",
    "road_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Volume\n",
       "2.0    1104722\n",
       "3.0     710955\n",
       "1.0     561599\n",
       "4.0     524573\n",
       "0.0     207230\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_data['Volume'].value_counts(dropna=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
